{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6b29da",
   "metadata": {},
   "source": [
    "1. **MultiHeadAttention**: This class implements the multi-head attention mechanism, a crucial component of the Transformer architecture. It computes attention for multiple attention heads independently, allowing the model to focus on different parts of the input simultaneously. The class contains linear layers to transform the input into Query (Q), Key (K), and Value (V) matrices, which are then split into multiple heads. Scaled dot-product attention is calculated for each head, and the outputs are concatenated and passed through a final linear layer.\n",
    "\n",
    "2. **PositionalEncoding**: This class is responsible for adding positional information to the input sequences. Since the self-attention mechanism in the Transformer architecture does not have any inherent notion of the position of words in the sequence, positional encodings are added to the input embeddings to provide information about the word positions. These encodings are crucial for the model to learn and understand the order of words in the input sequences. The class generates sinusoidal positional encodings and adds them to the input during the forward pass.\n",
    "\n",
    "3. **TransformerBlock**: This class represents a single layer in the Transformer architecture. Each layer contains a multi-head attention mechanism, layer normalization, a position-wise feed-forward network, and another layer normalization. Residual connections and dropout are also applied at appropriate points in the layer. The `TransformerBlock` class uses the `MultiHeadAttention` class to perform the multi-head attention operation and combines it with the other components to create a single Transformer layer.\n",
    "\n",
    "4. **Transformer**: This class combines the `TransformerBlock` layers and applies the `PositionalEncoding` to the input data. It encapsulates the full Transformer architecture by stacking multiple `TransformerBlock` layers and adding positional information to the input sequences. During the forward pass, the input data is first passed through the `PositionalEncoding` layer, and then sequentially through the stack of `TransformerBlock` layers. The output of the final `TransformerBlock` layer is returned as the output of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3924b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d53b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == d_model, \"Incompatible dimensions\"\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Apply linear layers\n",
    "        Q = self.query(query)\n",
    "        K = self.key(key)\n",
    "        V = self.value(value)\n",
    "\n",
    "        # Split d_model into num_heads * head_dim\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Calculate scaled dot-product attention\n",
    "        attn_weights = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Concatenate and pass through final linear layer\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.fc_out(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f5636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_seq_length, d_model)\n",
    "        pos = torch.arange(0, max_seq_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(pos * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:x.size(0), :].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b212eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Multi-head attention layer\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Position-wise feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        # Pass input through the multi-head attention layer\n",
    "        attn_output = self.attention(query, key, value)\n",
    "        \n",
    "        # Apply residual connection and layer normalization\n",
    "        out1 = self.norm1(query + self.dropout(attn_output))\n",
    "        \n",
    "        # Pass output through the feed-forward network\n",
    "        ff_output = self.feed_forward(out1)\n",
    "        \n",
    "        # Apply residual connection and layer normalization\n",
    "        out2 = self.norm2(out1 + self.dropout(ff_output))\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e73ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f24f8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "# Set the dimension of the input and output embeddings (model's hidden size)\n",
    "d_model = 64\n",
    "\n",
    "# Set the number of attention heads in the multi-head attention mechanism\n",
    "num_heads = 4\n",
    "\n",
    "# Set the dimension of the feed-forward network's hidden layer\n",
    "d_ff = 256\n",
    "\n",
    "# Set the number of layers in the Transformer architecture\n",
    "num_layers = 3\n",
    "\n",
    "# Set the maximum sequence length for input sequences\n",
    "max_seq_length = 10\n",
    "\n",
    "# Set the dropout rate for the dropout layers in the Transformer architecture\n",
    "dropout = 0.1\n",
    "\n",
    "# Set the batch size for input data\n",
    "batch_size = 1\n",
    "\n",
    "# Generate a random input tensor of shape (batch_size, max_seq_length, d_model)\n",
    "input_data = torch.randn(batch_size, max_seq_length, d_model)\n",
    "\n",
    "\n",
    "transformer = Transformer(d_model, num_heads, d_ff, num_layers, max_seq_length, dropout)\n",
    "output_data = transformer(input_data)\n",
    "print(output_data.shape)  # Expected output shape: (batch_size, max_seq_length, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baeac1b",
   "metadata": {},
   "source": [
    "- **MultiHeadAttention** is used within the `TransformerBlock` class to implement the multi-head attention mechanism.\n",
    "- **PositionalEncoding** and **TransformerBlock** are used within the `Transformer` class. The `PositionalEncoding` adds positional information to the input sequences, and the `TransformerBlock` represents a single layer in the Transformer architecture. Multiple `TransformerBlock` instances are stacked together to create the full Transformer model.\n",
    "- The `Transformer` class is instantiated and called in the end to build the model and process input data.\n",
    "\n",
    "In summary, the `Transformer` class encapsulates the full Transformer architecture, which includes the `PositionalEncoding` and multiple `TransformerBlock` layers. Each `TransformerBlock` layer uses the `MultiHeadAttention` class to perform the multi-head attention operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "576abe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─ModuleList: 1-1                        --\n",
      "|    └─TransformerBlock: 2-1             --\n",
      "|    |    └─MultiHeadAttention: 3-1      16,640\n",
      "|    |    └─LayerNorm: 3-2               128\n",
      "|    |    └─LayerNorm: 3-3               128\n",
      "|    |    └─Sequential: 3-4              33,088\n",
      "|    |    └─Dropout: 3-5                 --\n",
      "|    └─TransformerBlock: 2-2             --\n",
      "|    |    └─MultiHeadAttention: 3-6      16,640\n",
      "|    |    └─LayerNorm: 3-7               128\n",
      "|    |    └─LayerNorm: 3-8               128\n",
      "|    |    └─Sequential: 3-9              33,088\n",
      "|    |    └─Dropout: 3-10                --\n",
      "|    └─TransformerBlock: 2-3             --\n",
      "|    |    └─MultiHeadAttention: 3-11     16,640\n",
      "|    |    └─LayerNorm: 3-12              128\n",
      "|    |    └─LayerNorm: 3-13              128\n",
      "|    |    └─Sequential: 3-14             33,088\n",
      "|    |    └─Dropout: 3-15                --\n",
      "├─PositionalEncoding: 1-2                --\n",
      "=================================================================\n",
      "Total params: 149,952\n",
      "Trainable params: 149,952\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─ModuleList: 1-1                        --\n",
       "|    └─TransformerBlock: 2-1             --\n",
       "|    |    └─MultiHeadAttention: 3-1      16,640\n",
       "|    |    └─LayerNorm: 3-2               128\n",
       "|    |    └─LayerNorm: 3-3               128\n",
       "|    |    └─Sequential: 3-4              33,088\n",
       "|    |    └─Dropout: 3-5                 --\n",
       "|    └─TransformerBlock: 2-2             --\n",
       "|    |    └─MultiHeadAttention: 3-6      16,640\n",
       "|    |    └─LayerNorm: 3-7               128\n",
       "|    |    └─LayerNorm: 3-8               128\n",
       "|    |    └─Sequential: 3-9              33,088\n",
       "|    |    └─Dropout: 3-10                --\n",
       "|    └─TransformerBlock: 2-3             --\n",
       "|    |    └─MultiHeadAttention: 3-11     16,640\n",
       "|    |    └─LayerNorm: 3-12              128\n",
       "|    |    └─LayerNorm: 3-13              128\n",
       "|    |    └─Sequential: 3-14             33,088\n",
       "|    |    └─Dropout: 3-15                --\n",
       "├─PositionalEncoding: 1-2                --\n",
       "=================================================================\n",
       "Total params: 149,952\n",
       "Trainable params: 149,952\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the appropriate device (e.g., GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "# Display the summary of the model\n",
    "summary(transformer, input_size=(1, 512), batch_size=-1, device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2452bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─ModuleList: 1-1                        --\n",
      "|    └─TransformerBlock: 2-1             --\n",
      "|    |    └─MultiHeadAttention: 3-1      1,050,624\n",
      "|    |    └─LayerNorm: 3-2               1,024\n",
      "|    |    └─LayerNorm: 3-3               1,024\n",
      "|    |    └─Sequential: 3-4              2,099,712\n",
      "|    |    └─Dropout: 3-5                 --\n",
      "|    └─TransformerBlock: 2-2             --\n",
      "|    |    └─MultiHeadAttention: 3-6      1,050,624\n",
      "|    |    └─LayerNorm: 3-7               1,024\n",
      "|    |    └─LayerNorm: 3-8               1,024\n",
      "|    |    └─Sequential: 3-9              2,099,712\n",
      "|    |    └─Dropout: 3-10                --\n",
      "|    └─TransformerBlock: 2-3             --\n",
      "|    |    └─MultiHeadAttention: 3-11     1,050,624\n",
      "|    |    └─LayerNorm: 3-12              1,024\n",
      "|    |    └─LayerNorm: 3-13              1,024\n",
      "|    |    └─Sequential: 3-14             2,099,712\n",
      "|    |    └─Dropout: 3-15                --\n",
      "|    └─TransformerBlock: 2-4             --\n",
      "|    |    └─MultiHeadAttention: 3-16     1,050,624\n",
      "|    |    └─LayerNorm: 3-17              1,024\n",
      "|    |    └─LayerNorm: 3-18              1,024\n",
      "|    |    └─Sequential: 3-19             2,099,712\n",
      "|    |    └─Dropout: 3-20                --\n",
      "|    └─TransformerBlock: 2-5             --\n",
      "|    |    └─MultiHeadAttention: 3-21     1,050,624\n",
      "|    |    └─LayerNorm: 3-22              1,024\n",
      "|    |    └─LayerNorm: 3-23              1,024\n",
      "|    |    └─Sequential: 3-24             2,099,712\n",
      "|    |    └─Dropout: 3-25                --\n",
      "|    └─TransformerBlock: 2-6             --\n",
      "|    |    └─MultiHeadAttention: 3-26     1,050,624\n",
      "|    |    └─LayerNorm: 3-27              1,024\n",
      "|    |    └─LayerNorm: 3-28              1,024\n",
      "|    |    └─Sequential: 3-29             2,099,712\n",
      "|    |    └─Dropout: 3-30                --\n",
      "├─PositionalEncoding: 1-2                --\n",
      "=================================================================\n",
      "Total params: 18,914,304\n",
      "Trainable params: 18,914,304\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─ModuleList: 1-1                        --\n",
       "|    └─TransformerBlock: 2-1             --\n",
       "|    |    └─MultiHeadAttention: 3-1      1,050,624\n",
       "|    |    └─LayerNorm: 3-2               1,024\n",
       "|    |    └─LayerNorm: 3-3               1,024\n",
       "|    |    └─Sequential: 3-4              2,099,712\n",
       "|    |    └─Dropout: 3-5                 --\n",
       "|    └─TransformerBlock: 2-2             --\n",
       "|    |    └─MultiHeadAttention: 3-6      1,050,624\n",
       "|    |    └─LayerNorm: 3-7               1,024\n",
       "|    |    └─LayerNorm: 3-8               1,024\n",
       "|    |    └─Sequential: 3-9              2,099,712\n",
       "|    |    └─Dropout: 3-10                --\n",
       "|    └─TransformerBlock: 2-3             --\n",
       "|    |    └─MultiHeadAttention: 3-11     1,050,624\n",
       "|    |    └─LayerNorm: 3-12              1,024\n",
       "|    |    └─LayerNorm: 3-13              1,024\n",
       "|    |    └─Sequential: 3-14             2,099,712\n",
       "|    |    └─Dropout: 3-15                --\n",
       "|    └─TransformerBlock: 2-4             --\n",
       "|    |    └─MultiHeadAttention: 3-16     1,050,624\n",
       "|    |    └─LayerNorm: 3-17              1,024\n",
       "|    |    └─LayerNorm: 3-18              1,024\n",
       "|    |    └─Sequential: 3-19             2,099,712\n",
       "|    |    └─Dropout: 3-20                --\n",
       "|    └─TransformerBlock: 2-5             --\n",
       "|    |    └─MultiHeadAttention: 3-21     1,050,624\n",
       "|    |    └─LayerNorm: 3-22              1,024\n",
       "|    |    └─LayerNorm: 3-23              1,024\n",
       "|    |    └─Sequential: 3-24             2,099,712\n",
       "|    |    └─Dropout: 3-25                --\n",
       "|    └─TransformerBlock: 2-6             --\n",
       "|    |    └─MultiHeadAttention: 3-26     1,050,624\n",
       "|    |    └─LayerNorm: 3-27              1,024\n",
       "|    |    └─LayerNorm: 3-28              1,024\n",
       "|    |    └─Sequential: 3-29             2,099,712\n",
       "|    |    └─Dropout: 3-30                --\n",
       "├─PositionalEncoding: 1-2                --\n",
       "=================================================================\n",
       "Total params: 18,914,304\n",
       "Trainable params: 18,914,304\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the Transformer model\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "max_seq_length = 512\n",
    "dropout = 0.1\n",
    "\n",
    "model = Transformer(d_model, num_heads, d_ff, num_layers, max_seq_length, dropout)\n",
    "\n",
    "# Move the model to the appropriate device (e.g., GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Display the summary of the model\n",
    "summary(model, input_size=(1, 512), batch_size=-1, device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98eb33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
