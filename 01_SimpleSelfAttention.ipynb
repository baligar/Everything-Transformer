{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fffe33",
   "metadata": {},
   "source": [
    "This code defines a simple self-attention mechanism using a query, key, and value linear transformation. It demonstrates how to implement self-attention in PyTorch, taking input data of shape (batch_size, seq_len, d_model) and returning output data of the same shape.\n",
    "\n",
    "The `SimpleSelfAttention` class is a custom PyTorch module that can be integrated into your model as a layer. The `forward` function computes the dot product between the queries and keys, applies the softmax function to obtain attention weights, and then applies these weights to the values. This code can be a starting point for implementing more complex attention mechanisms or be used as-is for simple attention-based models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b608eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(SimpleSelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Compute the dot product between queries and keys\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (k.size(-1) ** 0.5)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        return attn_output\n",
    "\n",
    "# Example usage\n",
    "d_model = 64\n",
    "seq_len = 10\n",
    "batch_size = 1\n",
    "input_data = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "self_attention = SimpleSelfAttention(d_model)\n",
    "output_data = self_attention(input_data)\n",
    "print(output_data.shape)  # Expected output shape: (batch_size, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c711913",
   "metadata": {},
   "source": [
    "The `SimpleSelfAttention` provided earlier is just a small constituent mechanism of a transformer. It's an essential component, but not the complete architecture.\n",
    "\n",
    "A transformer is defined by its unique combination of components and mechanisms that work together to process sequential data. The fundamental aspects that make a transformer a \"transformer\" are:\n",
    "\n",
    "1. **The self-attention mechanism**: This is the core component that allows the model to weigh the importance of different tokens in the input sequence relative to each other. It replaces the recurrent and convolutional layers used in other sequence models, such as RNNs and CNNs.\n",
    "\n",
    "2. **The encoder-decoder architecture**: Transformers consist of an encoder that processes the input data and a decoder that generates the output based on the encoder's output and the target data. Both the encoder and decoder are composed of several layers of self-attention, position-wise feedforward networks, layer normalization, and residual connections.\n",
    "\n",
    "3. **Positional encoding**: Since self-attention mechanisms have no inherent notion of position or order of the input sequence, positional encoding is used to inject this information into the input embeddings. This allows the model to capture the relative positions of tokens in the sequence.\n",
    "\n",
    "4. **Layer normalization and residual connections**: These techniques help stabilize and improve the training of deep neural networks.\n",
    "\n",
    "So, a transformer is not just defined by the encoder-decoder architecture but by the combination of all these components working together to process sequential data efficiently and effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d36bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
